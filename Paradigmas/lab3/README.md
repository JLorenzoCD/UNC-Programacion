# ğŸ“° Feed Reader (with big data) - Lector de Feeds con procesamiento de grandes
volÃºmenes de datos en Java

## ğŸ§‘â€ğŸ’» Grupo 12 - NaN: Not a Name - UNC - Paradigmas - 2025

*Integrantes:*
- Canovas, JosÃ© Lorenzo
- Garione, Facundo Gabriel
- Pastore, Ezequiel David

---

## Conceptos importantes
1. **Describa el flujo de la aplicaciÃ³n** Â¿QuÃ© pasos sigue la aplicaciÃ³n desde
la lectura del archivo subscriptions.json hasta la obtenciÃ³n de las entidades
nombradas? Â¿CÃ³mo se reparten las tareas entre los distintos componentes del
programa?

    1. Se crea la **session de Spark**.
    2. Se crea un **contexto de Spark**.
    3. Se utiliza la clase `SubscriptionParser` para leer el archivo
    `subscriptions.json` y crear objeto `Subscription`, donde este ultimo
    almacena todas las suscripciones (`SingleSubscription`).
    4. Se recorre los *singleSubscriptions* y se almacena las URLs de sus feeds
    en conjunto con su tipo de formato a utilizar en el parser.
    5. Se utiliza el contexto de Spark para paralelizar y repartir porciones de
    URLs a los **Workers** para lo siguientes pasos.
    6. Se mapea cada URL a su correspondiente `Feed`, utilizando la clase
    `httpRequester` para obtenerlo y `SubscriptionParser` para parsearlo.
    7. Se agrupan todos los feeds de las suscripciones mediante collect en
    **Master**.
    8. Si el usuario no pasa como argumento `-ne` se muestran los Feeds y
    termina el programa, caso contrario se continua.
    9. Se selecciona una heuristica para luego obtener las entidades nombradas.
    10. Se obtienen todos los artÃ­culos de todos los feeds y utilizando el
    contexto de Spark se paraleliza los siguientes pasos para los **Workers**.
    11. Se procesan los artÃ­culos.
    12. Se crean las subclases respectivas para cada entidades nombrada.
    13. Se realiza la reducciÃ³n de las entidades nombradas para tener el conteo
    de la frecuencia de estas a lo largo de todos los artÃ­culos.
    14. Se agrupan todos las entidades nombradas mediante collect en **Master**.
    15. Se imprime por pantalla de forma amigable para el usuario los datos de
    todos las entidades nombradas.
    16. Se cierra el contexto y la session con Spark.


2. **Â¿Por quÃ© se decide usar Apache Spark para este proyecto?** Â¿QuÃ© necesidad
concreta del problema resuelve?

    > Se decide usar Apache Spark para poder trabajar con grandes volÃºmenes de
    feeds en el menor tiempo posible. Gracias a que varias tareas pesadas son
    independientes y se pueden realizar de forma paralela, como por ejemplo:
    **la obtenciÃ³n de los feeds mediante peticiones HTTP y su respectivo parseo**;
    **el procesamiento de los artÃ­culos y el conteo de las entidades nombradas**.

3. **Liste las principales ventajas y desventajas que encontrÃ³ al utilizar
Spark.**

    Ventajas:
    - AbstracciÃ³n.
    - Velocidad de procesamiento.
    - Rendimiento (mejor de lo que posiblemente pudiÃ©ramos lograr solos).
    - Escalabilidad (local/cluster).
    - APIs sencillas.
    - Procesamiento paralelo.

    Desventajas
    - Uso de memoria elevado.
    - Curva de aprendizaje (ya que es un framework).
    - Poca eficacia para pequeÃ±os volÃºmenes de datos.

4. **Â¿CÃ³mo se aplica el concepto de inversiÃ³n de control en este laboratorio?**
Explique cÃ³mo y dÃ³nde se delega el control del flujo de ejecuciÃ³n. Â¿QuÃ©
componentes deja de controlar el desarrollador directamente?

    Al utilizar Spark estamos cediendo el control de la ejecuciÃ³n de ciertas
    tareas, las cuales serian difÃ­ciles de implementar (y muy laborioso).

    Esta inversion de control se realiza en:
    1. La obtenciÃ³n de los feeds mediante peticiones HTTP y su respectivo parseo
    2. El procesamiento de los artÃ­culos y el conteo de las entidades nombradas

    Los componentes que se dejan de controlar directamente son:
    1. Los datos.
    2. La particiÃ³n de los datos.
    3. PlanificaciÃ³n de tareas.
    4. GestiÃ³n del paralelismo.
    5. AsignaciÃ³n de recursos.
    6. Manejo de errores.

5. **Â¿Considera que Spark requiere que el cÃ³digo original tenga una integraciÃ³n
tight vs loose coupling?**

    > Consideramos que Spark favorece una integraciÃ³n `loose coupling`, ya que
    uno se beneficia mucho de que los componentes de la aplicaciÃ³n estÃ©n
    modularizados, para que puedan evolucionar o reutilizarse de forma
    independiente.

6. **Â¿El uso de Spark afectÃ³ la estructura de su cÃ³digo original?** Â¿Tuvieron
que modificar significativamente clases, mÃ©todos o lÃ³gica de ejecuciÃ³n del
laboratorio 2?

    > No, se modifico casi nada del cÃ³digo original. Quitando la modificaciÃ³n
    del main, se extendieron varias subclases con la clase **Serializable** y
    corregimos un warning en cierta parte del cÃ³digo por el cambio de version
    de JAVA 24 a JAVA 11.


## ğŸ“Œ DescripciÃ³n

Este proyecto es una extensiÃ³n del lector de feeds RSS desarrollado en el
laboratorio 2, adaptado para trabajar con grandes volÃºmenes de datos utilizando
el framework Apache Spark. La aplicaciÃ³n permite leer mÃºltiples fuentes RSS
configuradas en un archivo subscriptions.json, descargar sus artÃ­culos y
realizar un anÃ¡lisis de texto para extraer entidades nombradas como personas,
lugares, organizaciones, productos, fechas, entre otras.

El objetivo principal es escalar el procesamiento de estos datos para que sea
eficiente incluso cuando se incrementa significativamente la cantidad de feeds y
artÃ­culos procesados. Spark nos permite ejecutar las tareas en paralelo, tanto
en la etapa de descarga como en la de anÃ¡lisis de entidades.

Este enfoque no solo mejora el rendimiento, sino que tambiÃ©n sienta las bases
para aplicar tÃ©cnicas mÃ¡s complejas.

El diseÃ±o modular y extensible de la aplicaciÃ³n permite incorporar fÃ¡cilmente
nuevos parsers de feeds, heurÃ­sticas de detecciÃ³n de entidades o funcionalidades
adicionales, manteniendo una estructura clara y desacoplada.

---

## ğŸ§  Decisiones de diseÃ±o

AdemÃ¡s de las decisiones tomadas en el laboratorio 2, se tomaron algunas
adicionales en este nuevo laboratorio:

- Casi todas las clases involucradas en la aplicaciÃ³n, fueron modificadas para
que extiendan de la clase Serializable (io.Serializable).
- Se elimina cÃ³digo repetido para la lectura, obtenciÃ³n y parseo de feeds.
- No se considerÃ³ necesaria la creaciÃ³n de nuevas clases o mÃ©todos.
---

## âš™ï¸ InstalaciÃ³n y ejecuciÃ³n

1.  AsegÃºrate de tener instalado el JDK (Java Development Kit) y el framework
Apache Spark.Puedes instalar jdk con:

    ```bash
    sudo apt install openjdk-11-jdk
    ```

    Y Apache Spark siguiendo los pasos de [este video](https://youtu.be/2iU71eYkFkU?si=9-zY_ejUMN3qdtf9)

2.  Clona este repositorio:

    ```bash
    git clone https://github.com/paradigmas25-g12/paradigmas25-g12-lab3.git
    cd paradigmas25-g12-lab3
    ```
3.  Compila el cÃ³digo Java mediante Makefile.
    -  Cambiar las variables `JAVA_PATH` y `SPARK_FOLDER` segÃºn su configuraciÃ³n.

4.  Ejecuta la aplicaciÃ³n.
    - Hay dos funciones de la aplicaciÃ³n distintas, con `make` se obtienen los
    feeds y se muestran por pantalla, mientras que con `make runHeuristic` se
    computan las entidades nombradas para todos los artÃ­culos de todos los feeds
    y se muestran por pantalla.

---

## âœ¨ Funcionalidades
- ğŸ”„ Lectura de Feeds RSS: Permite procesar mÃºltiples feeds desde distintas
URLs especificadas en un archivo de suscripciÃ³n.
- ğŸ§  Reconocimiento de Entidades Nombradas: Extrae entidades nombradas (como
personas, lugares, organizaciones, etc.) de los artÃ­culos mediante heurÃ­sticas
que son configurables.
- ğŸ“„ImpresiÃ³n legible de resultados: Tanto los artÃ­culos como las entidades
nombradas se muestran en consola con formato amigable.
- âš¡ Procesamiento distribuido con el framework Apache Spark: Utiliza el Spark
para realizar tareas de manera paralela, acelerando la obtenciÃ³n y anÃ¡lisis de
datos y permitiendo trabajar con grandes cantidades de datos.
- ğŸ“¦ Extensible: DiseÃ±o modular que facilita agregar nuevas heurÃ­sticas,
parsers, y tipos de feeds.
-----

## ğŸ—‚ï¸ Estructura del Proyecto
```
.
â”œâ”€â”€ README.md
â”œâ”€â”€ Makefile
â”œâ”€â”€ config
â”‚Â Â  â””â”€â”€ subscriptions.json
â”œâ”€â”€ lib
â”‚Â Â  â””â”€â”€ json-20250107.jar
â””â”€â”€ src
    â”œâ”€â”€ FeedReaderMain.java
    â”œâ”€â”€ feed
    â”‚Â Â  â”œâ”€â”€ Article.java
    â”‚Â Â  â””â”€â”€ Feed.java
    â”œâ”€â”€ httpRequest
    â”‚Â Â  â””â”€â”€ httpRequester.java
    â”œâ”€â”€ namedEntity
    â”‚Â Â  â”œâ”€â”€ Company.java
    â”‚Â Â  â”œâ”€â”€ Date.java
    â”‚Â Â  â”œâ”€â”€ Event.java
    â”‚Â Â  â”œâ”€â”€ NamedEntity.java
    â”‚Â Â  â”œâ”€â”€ NamedEntityFactory.java
    â”‚Â Â  â”œâ”€â”€ Other.java
    â”‚Â Â  â”œâ”€â”€ Product.java
    â”‚Â Â  â”œâ”€â”€ heuristic
    â”‚Â Â  â”‚Â Â  â”œâ”€â”€ Heuristic.java
    â”‚Â Â  â”‚Â Â  â”œâ”€â”€ QuickHeuristic.java
    â”‚Â Â  â”‚Â Â  â””â”€â”€ RandomHeuristic.java
    â”‚Â Â  â”œâ”€â”€ person
    â”‚Â Â  â”‚Â Â  â”œâ”€â”€ LastName.java
    â”‚Â Â  â”‚Â Â  â”œâ”€â”€ Name.java
    â”‚Â Â  â”‚Â Â  â”œâ”€â”€ Person.java
    â”‚Â Â  â”‚Â Â  â””â”€â”€ Title.java
    â”‚Â Â  â””â”€â”€ place
    â”‚Â Â      â”œâ”€â”€ City.java
    â”‚Â Â      â”œâ”€â”€ Country.java
    â”‚Â Â      â”œâ”€â”€ Direction.java
    â”‚Â Â      â”œâ”€â”€ OtherPlace.java
    â”‚Â Â      â””â”€â”€ Place.java
    â”œâ”€â”€ parser
    â”‚Â Â  â”œâ”€â”€ GeneralParser.java
    â”‚Â Â  â”œâ”€â”€ RedditParser.java
    â”‚Â Â  â”œâ”€â”€ RssParser.java
    â”‚Â Â  â””â”€â”€ SubscriptionParser.java
    â”œâ”€â”€ subscription
    â”‚Â Â  â”œâ”€â”€ Counter.java
    â”‚Â Â  â”œâ”€â”€ SingleSubscription.java
    â”‚Â Â  â””â”€â”€ Subscription.java
    â””â”€â”€ topic
        â”œâ”€â”€ Other.java
        â”œâ”€â”€ Topic.java
        â”œâ”€â”€ TopicFactory.java
        â”œâ”€â”€ culture
        â”‚Â Â  â”œâ”€â”€ Cinema.java
        â”‚Â Â  â”œâ”€â”€ Culture.java
        â”‚Â Â  â”œâ”€â”€ Music.java
        â”‚Â Â  â””â”€â”€ OtherCulture.java
        â”œâ”€â”€ politics
        â”‚Â Â  â”œâ”€â”€ International.java
        â”‚Â Â  â”œâ”€â”€ National.java
        â”‚Â Â  â”œâ”€â”€ OtherPolitics.java
        â”‚Â Â  â””â”€â”€ Politics.java
        â””â”€â”€ sports
            â”œâ”€â”€ Basket.java
            â”œâ”€â”€ F1.java
            â”œâ”€â”€ Football.java
            â”œâ”€â”€ Sports.java
            â””â”€â”€ Tennis.java
```
-----

## ğŸ‘¥ CrÃ©ditos

Trabajo realizado para la materia **"Paradigmas de la ProgramaciÃ³n" - FAMAF -
UNC** ğŸ›ï¸
